{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Utils for NLP\n",
    "\n",
    "The document is structured:\n",
    "\n",
    "[Brief explanation]<br>\n",
    "[Function]<br>\n",
    "[Example of use]<br>\n",
    "\n",
    "All the functions comes with their correspondent docstring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Corpus Data\n",
    "\n",
    "Just read your data :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename,encoding='utf-8'):\n",
    "    '''\n",
    "    Read the file named 'filename' and returns it as str\n",
    "    args:\n",
    "    filename: The file name\n",
    "    encoding: default 'utf-8'\n",
    "    return:\n",
    "    A string with the contents of the file\n",
    "    '''\n",
    "    corpus_raw = u\"\"\n",
    "    with codecs.open(filename,'r','utf-8') as file:\n",
    "        corpus_raw += file.read()\n",
    "    return corpus_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sample (First 140 characters):\n",
      "This edition contains the complete text of the original hardcover edition.\n",
      "\n",
      "NOT ONE WORD HAS BEEN OMITTED.\n",
      "\n",
      "A CLASH OF KINGS\n",
      "\n",
      "A Bantam Spect ...\n"
     ]
    }
   ],
   "source": [
    "corpus = read_data('got1.txt')\n",
    "print('Data sample (First 140 characters):\\n%s ...' %corpus[0:140])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read several files and merge them into corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_merge_files(filenames,encoding='utf-8'):\n",
    "    '''\n",
    "    Read and merge the files which names are contained in 'filenames' list and returns them as str\n",
    "    args:\n",
    "    filenames: A list of names in string\n",
    "    encoding: default 'utf-8'\n",
    "    return:\n",
    "    A string with the contents of the files merged\n",
    "    '''\n",
    "    corpus_raw = u\"\"\n",
    "    for filename in filenames:\n",
    "        print('Reading %s' % filename)\n",
    "        with codecs.open(filename,'r','utf-8') as file:\n",
    "            corpus_raw += file.read()\n",
    "        print('Corpus in now %d characters long' % len(corpus_raw))\n",
    "        print()\n",
    "    return corpus_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading got1.txt\n",
      "Corpus in now 1770659 characters long\n",
      "\n",
      "Reading tiny_shakespeare.txt\n",
      "Corpus in now 2886053 characters long\n",
      "\n",
      "Data sample (First and last 140 characters):\n",
      " This edition contains the complete text of the original hardcover edition.\n",
      "\n",
      "NOT ONE WORD HAS BEEN OMITTED.\n",
      "\n",
      "A CLASH OF KINGS\n",
      "\n",
      "A Bantam Spect // ing, moving,\n",
      "And yet so fast asleep.\n",
      "\n",
      "ANTONIO:\n",
      "Noble Sebastian,\n",
      "Thou let'st thy fortune sleep--die, rather; wink'st\n",
      "Whiles thou art waking.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filenames = ['got1.txt','tiny_shakespeare.txt']\n",
    "corpus = read_and_merge_files(filenames)\n",
    "print('Data sample (First and last 140 characters):\\n %s // %s' %(corpus[0:140],corpus[-140:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract sentences from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_sentences(corpus):\n",
    "    '''\n",
    "    Extracts all the sentences present in a corpus. Uses NLTK tokenizer for english.\n",
    "    args:\n",
    "    corpus: A 'str' with the corpus to extract sentences from\n",
    "    return:\n",
    "    The corpus represented as a list of sentences.\n",
    "    '''\n",
    "    assert type(corpus) is str, \"corpus is not a string: %r\" % corpus\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(corpus)\n",
    "    return raw_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 sentences as sample:\n",
      " ['This edition contains the complete text of the original hardcover edition.', 'NOT ONE WORD HAS BEEN OMITTED.', 'A CLASH OF KINGS\\n\\nA Bantam Spectra Book\\n\\nPUBLISHING HISTORY\\n\\nBantam Spectra hardcover edition published February 1999\\n\\nBantam Spectra paperback edition / September 2000\\n\\nSPECTRA and the portrayal of a boxed “s” are trademarks of Bantam Books, a division of Random House, Inc.\\n\\nAll rights reserved.', 'Copyright © 1999 by George R. R. Martin.', 'Maps by James Sinclair.']\n"
     ]
    }
   ],
   "source": [
    "sentences = extract_sentences(corpus=corpus)\n",
    "print('The first 5 sentences as sample:\\n %s' %(sentences[0:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract word from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_words(corpus):\n",
    "    '''\n",
    "    Extracts all the words present in a corpus. Removes everything except [a-z] & [A-Z]\n",
    "    args:\n",
    "    corpus: A 'str' with the corpus to extract words from\n",
    "    return:\n",
    "    The corpus represented as a list of words.\n",
    "    '''\n",
    "    assert type(corpus) is str, \"corpus is not a string: %r\" % corpus\n",
    "    clean = re.sub(\"[^a-zA-z]\",\" \",corpus)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 words as sample:\n",
      " ['This', 'edition', 'contains', 'the', 'complete', 'text', 'of', 'the', 'original', 'hardcover']\n"
     ]
    }
   ],
   "source": [
    "vocab = extract_words(corpus=corpus)\n",
    "print('The first 10 words as sample:\\n %s' %(vocab[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exract characters from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_characters(corpus):\n",
    "    '''\n",
    "    Extracts all the characters in a corpus.\n",
    "    args:\n",
    "    corpus: A 'str' with the corpus to extract words from\n",
    "    return:\n",
    "    The corpus represented as a list of characters.\n",
    "    '''\n",
    "    chars = []\n",
    "    for line in corpus:\n",
    "        chars.extend(line)\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 140 characters as sample:\n",
      " ['T', 'h', 'i', 's', ' ', 'e', 'd', 'i', 't', 'i', 'o', 'n', ' ', 'c', 'o', 'n', 't', 'a', 'i', 'n', 's', ' ', 't', 'h', 'e']\n"
     ]
    }
   ],
   "source": [
    "characters = extract_characters(corpus)\n",
    "print('The first 140 characters as sample:\\n %s' %(characters[0:25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract unique word from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_unique_words(corpus):\n",
    "    '''\n",
    "    Generates the words vocabulary of the corpus.\n",
    "    Extracts every unique single words present in a corpus. Removes everything except [a-z] & [A-Z]\n",
    "    args:\n",
    "    corpus: A 'str' with the corpus to extract words from\n",
    "    return:\n",
    "    A list of str with all the unique present words in the corpus\n",
    "    '''\n",
    "    assert type(corpus) is str, \"corpus is not a string: %r\" % corpus\n",
    "    clean = re.sub(\"[^a-zA-z]\",\" \",corpus)\n",
    "    words = clean.split()\n",
    "    return list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary (words): 22640\n",
      "The first 10 words as sample:\n",
      " ['Ooooooooooooooooooo', 'arraign', 'removedness', 'dowery', 'hell', 'Orphan', 'Assuming', 'Leobald', 'jawed', 'reluctant']\n"
     ]
    }
   ],
   "source": [
    "unique_words = extract_unique_words(corpus)\n",
    "print('Size of vocabulary (words): %d' %len(unique_words))\n",
    "print('The first 10 words as sample:\\n %s' %(unique_words[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract unique characters from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_unique_characters(corpus):\n",
    "    '''\n",
    "    Generates the char vocabulary of the corpus.\n",
    "    Extracts every unique single character present in a corpus. \n",
    "    args:\n",
    "    corpus: A 'str' with the corpus to extract characters from\n",
    "    return:\n",
    "    A list of str with all the unique present characters in the corpus\n",
    "    '''\n",
    "    return list(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary (characters): 85\n",
      "The first 10 characters as sample:\n",
      " ['O', 'Y', 'C', 'b', '!', 'h', '.', '\\n', 'L', ':']\n"
     ]
    }
   ],
   "source": [
    "unique_characters = extract_unique_characters(corpus)\n",
    "print('Size of vocabulary (characters): %d' %len(unique_characters))\n",
    "print('The first 10 characters as sample:\\n %s' %(unique_characters[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab to ID & ID to Vocab DICTIONARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dictionaries(vocab):\n",
    "    '''\n",
    "    Creates Dictionary 'id_to_vocab' and Reverse Dictionary 'vocab_to_id' from a given vocab (chars or words)\n",
    "    args:\n",
    "    vocab: A list with every unique single word or character present in a corpus. Could be the output of 'extract_unique_words' or 'extract_unique_characters' functions.\n",
    "    return:Dictionaries: [id_to_vocab,vocab_to_id]\n",
    "    '''\n",
    "    id_to_vocab = {i:ch for i,ch in enumerate(vocab)}\n",
    "    vocab_to_id = {ch:i for i,ch in enumerate(vocab)}\n",
    "    return id_to_vocab,vocab_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With **words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 4 elements of dict [id_to_vocab] as sample:\n",
      "0 Ooooooooooooooooooo\n",
      "1 arraign\n",
      "2 removedness\n",
      "3 dowery\n",
      "\n",
      "The first 4 elements of dict [vocab_to_id] as sample:\n",
      "Ooooooooooooooooooo 0\n",
      "flags 11305\n",
      "arraign 1\n",
      "dowery 3\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import collections\n",
    "\n",
    "id_to_vocab,vocab_to_id = create_dictionaries(vocab=unique_words)\n",
    "\n",
    "print('The first 4 elements of dict [id_to_vocab] as sample:')\n",
    "x = itertools.islice(id_to_vocab.items(), 0, 4)\n",
    "for key, value in x:\n",
    "    print (key, value)\n",
    "    \n",
    "print('\\nThe first 4 elements of dict [vocab_to_id] as sample:')\n",
    "y = itertools.islice(vocab_to_id.items(), 0, 4)\n",
    "for key, value in y:\n",
    "    print (key, value)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With **characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 4 elements of dict [id_to_char] as sample:\n",
      "0 O\n",
      "1 Y\n",
      "2 C\n",
      "3 b\n",
      "\n",
      "The first 4 elements of dict [char_to_id] as sample:\n",
      "O 0\n",
      "Y 1\n",
      "C 2\n",
      "! 4\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import collections\n",
    "\n",
    "id_to_char,char_to_id = create_dictionaries(unique_characters)\n",
    "\n",
    "print('The first 4 elements of dict [id_to_char] as sample:')\n",
    "x = itertools.islice(id_to_char.items(), 0, 4)\n",
    "for key, value in x:\n",
    "    print (key, value)\n",
    "    \n",
    "print('\\nThe first 4 elements of dict [char_to_id] as sample:')\n",
    "y = itertools.islice(char_to_id.items(), 0, 4)\n",
    "for key, value in y:\n",
    "    print (key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus words as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_corpus_as_integers(corpus,vocab_to_id,level='words'):\n",
    "    '''\n",
    "    Generate the integers representation of a corpus with a provided vocab_to_id dictictionary (that could be the output of 'create_dictionaries' function)\n",
    "    args:\n",
    "    corpus: The corpus as 'str' to generate the representation from\n",
    "    vocab_to_id: A dictionary to map the representation {'vocab_element':'id'}\n",
    "    level: Two possible values ['words','chars']. Representation of the corpus by words or by characters. Default 'words'\n",
    "    return:\n",
    "    The corpus as a list of int\n",
    "    '''\n",
    "    assert level == 'words' or level=='chars', 'level must be \"words\" or \"chars\"'\n",
    "    corpus_as_int = []\n",
    "    \n",
    "    if(level == 'words'):\n",
    "        clean = re.sub(\"[^a-zA-z]\",\" \",corpus)\n",
    "        words = clean.split()\n",
    "\n",
    "        for word in words:\n",
    "            corpus_as_int.append(vocab_to_id[word])\n",
    "\n",
    "    elif(level == 'chars'):\n",
    "        chars = []\n",
    "        for line in corpus:\n",
    "            chars.extend(line)\n",
    "        for char in chars:\n",
    "            corpus_as_int.append(vocab_to_id[char])\n",
    "            \n",
    "    return corpus_as_int\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With **words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same text: as integers, as words:\n",
      "[14093, 7679, 3641, 8556, 18213, 1661, 13203, 8556, 8316, 20278]\n",
      "['This', 'edition', 'contains', 'the', 'complete', 'text', 'of', 'the', 'original', 'hardcover']\n"
     ]
    }
   ],
   "source": [
    "corpus_by_words_as_int = extract_corpus_as_integers(corpus,vocab_to_id)\n",
    "print('Same text: as integers, as words:')\n",
    "print(corpus_by_words_as_int[0:10])\n",
    "list_words_as_words = []\n",
    "for ix in corpus_by_words_as_int[0:10]:\n",
    "    list_words_as_words.append(id_to_vocab[ix])\n",
    "print(list_words_as_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With **characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same text, as integers, as words:\n",
      "[50, 5, 52, 59, 17, 66, 28, 52, 41, 52]\n",
      "['T', 'h', 'i', 's', ' ', 'e', 'd', 'i', 't', 'i']\n"
     ]
    }
   ],
   "source": [
    "corpus_by_chars_as_int = extract_corpus_as_integers(corpus,char_to_id,level='chars')\n",
    "print('Same text, as integers, as words:')\n",
    "print(corpus_by_chars_as_int[0:10])\n",
    "list_chars_as_chars = []\n",
    "for ix in corpus_by_chars_as_int[0:10]:\n",
    "    list_chars_as_chars.append(id_to_char[ix])\n",
    "print(list_chars_as_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build dataset \n",
    "\n",
    "Google style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def build_dataset(words, vocabulary_size):\n",
    "    '''\n",
    "    Creates a dataset, given the corpus as a list of words and a vocabulary size representing the number of words to retain.\n",
    "    Replaces unknown words with <unk> token.\n",
    "    The dataset is the corpus represented by words as integers.\n",
    "    args:\n",
    "    words: The corpus as a list of words\n",
    "    vocabulary_size: The number of words to retain\n",
    "    returns:\n",
    "    data: The corpus, as a list of words by its integer representation. '0' means 'unknown word'\n",
    "    count: list that holds the counting with pairs like {'word':count}. Includes the <unk> count as first element.\n",
    "    dictionary: A 'vocab_to_id' dictionary like {'word':id}. By construction, lower id means most common.\n",
    "    reverse_dictionary An 'id_to_vocab' dictionary like {id,'word'}. By construction, lower id means most common.\n",
    "    '''\n",
    "    data = [] #Here we'll return the corpus as integers\n",
    "    \n",
    "    count = [['UNK', -1]] #Initialize the 'unknown' counter with -1\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1)) #Store in count pairs like {'word':appearances}\n",
    "    \n",
    "    #Creates a dictionary with the unique words like {'word':id}\n",
    "    #By construction, the most common words have the lowest id\n",
    "    dictionary = dict()\n",
    "    #Construction: Iterates the count from most common to uncommon. The dictionary length increases at each step.\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary) \n",
    "  \n",
    "    #Iterates over the words\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary: #If word is in dictionary, then is a 'vocabulary_size' common word\n",
    "            index = dictionary[word] #The index is given by the dictionary\n",
    "        else:\n",
    "            index = 0  #Add to the first entry of the list, where we store unknown words => dictionary['UNK'] \n",
    "            unk_count += 1 #Update the unknown word count\n",
    "        data.append(index) #Append to data\n",
    "        \n",
    "    #Now data is a representation of the corpus, by words, as integers. Note that the presence of '0' means 'unknown word'\n",
    "    \n",
    "    \n",
    "    count[0][1] = unk_count #Update the unknown count in 'count'\n",
    "    \n",
    "    #Creates dictionary (vocab_to_id) and reverse dictionary\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 4 elements of list that holds the counting [count] as sample:\n",
      " [['UNK', 16756], ('the', 21809), ('and', 12928), ('to', 11326)]\n",
      "\n",
      "The first 4 elements of dict [dictionary] as sample:\n",
      "The 20\n",
      "greeted 5264\n",
      "Dalbridge 3830\n",
      "VALERIA 3482\n",
      "\n",
      "The first 4 elements of dict [reverse_dictionary] as sample:\n",
      "0 UNK\n",
      "1 the\n",
      "2 and\n",
      "3 to\n",
      "\n",
      "Same text: as integers, as words:\n",
      "[176, 8268, 9223, 1, 6338, 0, 4, 1, 0, 0]\n",
      "['This', 'edition', 'contains', 'the', 'complete', 'UNK', 'of', 'the', 'UNK', 'UNK']\n"
     ]
    }
   ],
   "source": [
    "#Because the unique words size in the example corpus is ~15.000, with a vocabulary_size less than that, we'll have unknown words\n",
    "data,count,dictionary,reverse_dictionary = build_dataset(extract_words(corpus),vocabulary_size=10000)\n",
    "\n",
    "print('The first 4 elements of list that holds the counting [count] as sample:\\n %s\\n' % count[0:4])\n",
    "\n",
    "print('The first 4 elements of dict [dictionary] as sample:')\n",
    "x = itertools.islice(dictionary.items(), 0, 4)\n",
    "for key, value in x:\n",
    "    print (key, value)\n",
    "    \n",
    "print('\\nThe first 4 elements of dict [reverse_dictionary] as sample:')\n",
    "x = itertools.islice(reverse_dictionary.items(), 0, 4)\n",
    "for key, value in x:\n",
    "    print (key, value)\n",
    "\n",
    "print('\\nSame text: as integers, as words:')\n",
    "print(data[0:10])\n",
    "list_words_as_words = []\n",
    "for ix in data[0:10]:\n",
    "    list_words_as_words.append(reverse_dictionary[ix])\n",
    "print(list_words_as_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Word2Vec Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(data,batch_size=16,num_skips=1,skip_window=1):\n",
    "    \"\"\"Return batch and label from data\n",
    "    args:\n",
    "    data: A list of 'int'\n",
    "     \n",
    "    \"\"\" \n",
    "    global data_index\n",
    "    data_index = 0\n",
    "    assert batch_size % num_skips == 0 \n",
    "    assert num_skips < 2*skip_window \n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size),dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size,1),dtype=np.int32)\n",
    "    \n",
    "    span = 2 * skip_window + 1 # Total length, target + context => [skip_window target skip_window]\n",
    "    buffer = collections.deque(maxlen=span) #A list-like sequence with maxlen, if more appended, the first dissappears\n",
    "    \n",
    "    #Add the the words to buffer [skip_window target skip_window]\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "        \n",
    "    #A más skips menos vueltas\n",
    "    for i in range(batch_size // num_skips): \n",
    "        target = skip_window #Target at the center\n",
    "        targets_to_avoid = [skip_window]\n",
    "        \n",
    "        for j in range(num_skips):#For each skip\n",
    "            while target in targets_to_avoid:\n",
    "                target = np.random.randint(0,span-1)\n",
    "            targets_to_avoid.append(target)\n",
    "            \n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j,0] = buffer[target]\n",
    "        \n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "        \n",
    "    #Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With **words**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch,labels = generate_batch(data=corpus_by_words_as_int,batch_size=16,num_skips=1,skip_window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated batch is:\n",
      "[ 7679  3641  8556 18213  1661 13203  8556  8316 20278  7679 19721 10721\n",
      "  9948    28 12847 15899]\n",
      "\n",
      "Translated to words are:\n",
      "['edition', 'contains', 'the', 'complete', 'text', 'of', 'the', 'original', 'hardcover', 'edition', 'NOT', 'ONE', 'WORD', 'HAS', 'BEEN', 'OMITTED']\n",
      "\n",
      "The generated labels for the batch are:\n",
      "[14093  7679  3641  8556 18213  1661 13203  8556  8316 20278  7679 19721\n",
      " 10721  9948    28 12847]\n",
      "\n",
      "Translated to words are:\n",
      "['This', 'edition', 'contains', 'the', 'complete', 'text', 'of', 'the', 'original', 'hardcover', 'edition', 'NOT', 'ONE', 'WORD', 'HAS', 'BEEN']\n"
     ]
    }
   ],
   "source": [
    "print('The generated batch is:\\n%s\\n' %batch)\n",
    "print('Translated to words are:')\n",
    "translation = []\n",
    "for ix in batch:\n",
    "    translation.append(id_to_vocab[ix])\n",
    "print(translation)\n",
    "\n",
    "print('\\nThe generated labels for the batch are:\\n%s\\n' %labels.reshape(-1))\n",
    "print('Translated to words are:')\n",
    "translation = []\n",
    "for ix in labels.reshape(-1):\n",
    "    translation.append(id_to_vocab[ix])\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With **characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch,labels = generate_batch(data=corpus_by_chars_as_int,batch_size=16,num_skips=1,skip_window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated batch is:\n",
      "[ 5 52 59 17 66 28 52 41 52 33 83 17 61 33 83 41]\n",
      "\n",
      "Translated to characters are:\n",
      "['h', 'i', 's', ' ', 'e', 'd', 'i', 't', 'i', 'o', 'n', ' ', 'c', 'o', 'n', 't']\n",
      "\n",
      "The generated labels for the batch are:\n",
      "[50  5 52 59 17 66 28 52 41 52 33 83 17 61 33 83]\n",
      "\n",
      "Translated to words are:\n",
      "['T', 'h', 'i', 's', ' ', 'e', 'd', 'i', 't', 'i', 'o', 'n', ' ', 'c', 'o', 'n']\n"
     ]
    }
   ],
   "source": [
    "print('The generated batch is:\\n%s\\n' %batch)\n",
    "print('Translated to characters are:')\n",
    "translation = []\n",
    "for ix in batch:\n",
    "    translation.append(id_to_char[ix])\n",
    "print(translation)\n",
    "\n",
    "print('\\nThe generated labels for the batch are:\\n%s\\n' %labels.reshape(-1))\n",
    "print('Translated to words are:')\n",
    "translation = []\n",
    "for ix in labels.reshape(-1):\n",
    "    translation.append(id_to_char[ix])\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
